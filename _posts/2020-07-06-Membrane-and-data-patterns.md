---
layout: post
title: Membrane and data patterns
---

The in-memory key-value store example of Laminar use I mentioned previously is now available on GitHub:  [Membrane](https://github.com/jmdisher/Membrane).  This can run stand-alone or attached to a Laminar cluster but only supports programmable topics when back-ending onto Laminar (as it doesn't have its own AVM runtime).

It works pretty well and does a good job of demonstrating Laminar but also the "field, not the entity" data modelling philosophy.  In fact, this philosophy is what allows it to more effectively leverage Laminar, in general.  This is because each field in the "entity" (which can be rendered as a JSON document through one of Membrane's REST end-points) directly maps to a single Laminar topic.  This means that those fields can have programs applied to them, which will produce the consequences which Membrane uses to produce its projection.  In fact, since Membrane doesn't do anything with the data coming through its write path, the write could be arbitrarily complex instructions to the program so long as the consequences produced can be interpreted as the corresponding type.  In a real system, this aspect would need to be extensible to account for complexity in the consequences but the example proves that it works.

The next step, aside from on-going [Laminar whitepaper work](https://github.com/jmdisher/Laminar/wiki/Draft-White-Paper), is to further explore data modelling patterns to solve more complex cases to see what options exist.  The 3 examples I am using to initiate the ideas are:

1. Chatroom
1. Weblog
1. Collaborative text editor

This is a small set of problems but they each represent a very different kind of data relationship:  A chatroom is basically just an array, a weblog is essentially a tree (or array of trees), and the text editor is more difficult to boil down to one idea (beyond ultimately being a bag of bytes) as it is the result of a sequence of operational transforms.


## Chatroom

The chatroom is probably the simplest case.  The room would be a single topic where the keys are the participants and the values being whatever they said.  This is pretty simple to project, although not as completely obvious as a key-value store.  Still, given that history isn't as important as recent content, it is effectively just a sliding window over the consequences generated by the topic, directly.  An interesting detail of how blind compaction would apply to this is that it would reduce older content to just being the last thing that user said, which is probably about right.  Deleting the user's key would also allow them to be compacted out of history.

No real cleverness is required here although a potential use of programmability would be for throttling and making sure the user isn't too out-of-sync:  Pass in the most recently read offset with the write and the program could reject it if too far behind.  A somewhat odd feature, but one which is possible.


## Weblog

The weblog case introduces some complexity but still makes sense within the broader design.  Most likely, a single post would be represented as a topic and the content and comments within it would be the keys.  This would allow for all comments to be implicitly deleted when the post is deleted, since it would take the entire topic with it.

The comments would just need IDs (which could potentially be assigned by a program in the topic or just UUID generated by the posting user) and each would include their parent ID, along with the comment.  This could further be extended to make sure the parent still exists by using a cooperative cache approach like the [balance transfer example](https://github.com/jmdisher/Laminar/blob/master/bridge/src/com/jeffdisher/laminar/contracts/AccountBalanceValidation.java), but that is likely overkill (especially since it couldn't automatically prune after delete, nor would most users want that).

This means that editing comments or the entire post would just update the same key, meaning that compaction would nicely delete the old values and the only special interpretation required is looking at the parent ID reference (note that most comment systems, today, don't even present this sort of hierarchical relationship).

The part which doesn't obviously fit is the post content, itself, since many weblog entries are more than 64 KiB (the maximum Laminar data size).  However, going much larger than that is kind of an odd thing to shove through such a data structure, so we can probably use the tried-and-tested solution:  Store the content elsewhere and only write the reference.  Sure, it requires using 2 systems, thus breaking cross-architecture transactional boundaries, but this case wouldn't care:  If you fail to write the data, don't write the reference.

There are several ways to do this but a "modern" approach would probably be to write it to a content-addressed distributed file system, like IPFS.  When the reader sees the reference, it could just pin it.  If it sees an edit (which is just a new PUT with a different reference), it could just pin the new reference and unpin the old one.  This isn't a new problem and large files generally shouldn't be put into any kind of database for a whole host of reasons.


## Collaborative text editor

Now, we get onto the tricky case.  While the way to build such systems is well known (a stream of operational transforms and rules for how one can be applied to the document, but also meta-data - like user cursors), fitting them into a generalized data model is not obvious.  Correspondingly, this section is less about a solution, as opposed to spit-balling ideas.  Bear with me...

Part of the problem is that the stream of operational transforms is basically just a very simple version of what Laminar already is, albeit with a very specific interpretation of the mutations being applied.  I suspect that this means that the solution would be to emulate an event store _inside_ an event store.

The simplest way to do this is just to store a single document in a topic and use the same key for every operation.  These operations would be the usual set of "change region X to content Y", "change cursor/selection", etc, and compaction could never be enabled for the topic since all of them would be required for building the data, in the projection.

This could be slightly improved by embedding a program to split between content changes and cursor movement:  The program assigns a new key to each content change but uses the same key for each cursor update.  This would be similar but would allow compaction to eliminate cursor movement from the history, since it doesn't really matter.  This would be similar to using an out-of-band mechanism for this data, but would avoid cross-architecture sync issues.

It would also be possible to build a kind of "external snapshot" to allow more intermediate operations to be purged (if that is desired) such that 3 keys could be used:  Snapshot, content change, and cursor change.  This is unpleasant, though, as it requires an external agent to build this snapshot (probably using the aforementioned IPFS reference approach), submit that with a reference to a specific content change action, and then allow compaction to proceed only until that point.  Possible and would reduce space used in the cluster, but pretty ugly as it requires a special external agent and special knowledge in the compactor.

Really, the simplest approach would probably be closest to the first idea and just use 2 keys:  Content changes and cursor changes.  From here, ensure that the compactor can be configured to treat a set of keys as uncompactable and mark the content change as one of those.  It is still a lot of long-lived overhead for the document, but it does capture the entire scope of the document in a 4-dimensional sense, just avoiding the noise of the cursors.

A potentially valuable use of a programmable topic here would be to update all incoming changes so that they all occur after each other, not at varying levels of synchronization with the state of the document (as this is probably the "tricky part" in building such an editor's data model).  This could use a fixed back-log of mutation offset changes and reject mutations which come in from before those.

It is a challenging problem to fit into this model, so it is an interesting challenge and a test of what can be considered.


## Why data model musings?

The reason for this tangent is actually part of a broader idea I have been playing with, which can leverage Laminar, but also potentially solve some other problems related to modern software development.  I want to look at these 3 data model examples as a challenge for Laminar, because that would implicitly solve them for a broader issue of how sufficiently expressive declarative data modelling can fit into declarative UI description and allow the imperative control logic to be written between these relatively clean boundaries.  To put it another way, I want to come up with a way of reducing things like UI and data model to declarative resource files only referenced by external code to implement business logic, alone.

I know that this has been done a few times before (MIDP, Cocoa Bindings, etc), but I haven't seen one recently which wasn't actually just framework hell, once you get under the covers.  I want this to be a data file or library which they can choose how to use, not half-baked "magic" which they need to somehow fit within (and the re-write from scratch when a new framework becomes trendy the following year).

It is ambitious but interesting and should be doable since, as mentioned, this has been done before.


[Comments and Discussion](https://github.com/jmdisher/Laminar-blog/issues/17)

---
layout: post
title: The Trials and Tribulations of Client Reconnection
---

A part of distributed systems such as Laminar (those with a client-server design, from the client's perspective) is how to manager **client reconnection**.  This can be reconnecting to the same server after a network interruption but more generally should be viewed as the reconnection to a **different server in the cluster**, after a fail-over.

The problem is related to how to produce a seemless experience for the user's code on the client side and **never drop or duplicate any of their in-flight messages**.  The connection can drop at any moment and some of the client's messages which it hasn't seen commit may have actually committed, may have at least been received (which the client may know), and some may not have made it to the server before the disconnection.  What is more, if the lead server was suddenly taken offline, it may have seen a message which the rest of the cluster has not.  Client reconnection is meant to solve all of these issues.  In an ideal world, all the client code sees is that a few messages might take longer to commit than the ones before or after, but everything should be **logically equivalent to the disconnect never happening**.

In the past, I have worked on a project which solved this problem by having the client send the offset it most recently saw commit (via an ack from the server) in any new messages it sends.  The server then needs to maintain a copy of all the responses to the client's messages going back to at least that point.  This sounds simple enough but remember that _every_ server needs this information.  This means that additional meta-data needs to be going between the server nodes to keep this historical state in sync.  The nodes are all deterministic so they don't need to pass the actual response, just this trailing offset, between them.  This allows them all to keep an **eventually-consistent snapshot of the state of all clients' responses which could possibly be required**.  If the cluster has a persistent/restartable mode, then these also need to be durable.  While this approach does work, it adds many more moving parts which I was eager to avoid in Laminar.

In Laminar, I used a different approach which generalizes this handling into the **same mechanism used to support downstream peer synchronization**.  That is, the reconnecting clients ask the server to read the committed list of committed `MutationRecord` instances to re-respond with the state of any of these in-flight messages.  This works, and **avoids the need to build these extra network-visible concepts** across the cluster.  It also has the interesting consequence that a client can theoretically resync its state from any point in time since the cluster started (obviously not true if compaction were added).

The downside of this (aside from this extra IO load on the leader) is that it requires storing more information which wouldn't normally be required along with the `MutationRecord` instances (which also means reaching past this information when syncing a downstream node - `sendfile` won't be an option unless the downstream filters it out).  This is why `CommittedMutationRecord` now exists, **storing whatever additional meta-data might be required** for a reconnecting client in that persistent data stream.

I am not sure I am happy with this approach but it does have a certain elegance in that it **will never fail and doesn't introduce new concepts** (just pollutes an existing one).  Another possibility I toyed with was making a fixed-size buffer of `N` most recent commit records, on all servers, and defining a failure mode where the client can be told that a commit did happen, and what its offset was, but won't know the outcome of it.  This seemed like a bad idea, though, since the client probably requires that data.  Further, if the benefit of Laminar is knowing the result of programmable topic commit checks, then making that not always reliable is probably defeating the purpose (assuming the result is used - just knowing the commit check _did_ run may be all that matters).  Still, I really don't like it when systems don't provide a way for you to know what is happening, so allowing such a blind spot is probably not a good idea.

In any case, the current mechanism is simple and keeps the network chatter simple and sparse, which seems like a good idea.  We will see what implications this has, in the future.
